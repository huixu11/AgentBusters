#!/usr/bin/env python3
"""
æ‰¹é‡è¿è¡Œå¤šé…ç½®å®éªŒ

ç”¨äºç³»ç»Ÿæ€§åœ°æµ‹è¯•ä¸åŒ LLM æ¨¡å‹ã€ä»»åŠ¡æ•°é‡ã€æŠ½æ ·ç­–ç•¥ç­‰é…ç½®ä¸‹çš„ benchmark ç»“æœã€‚
ç»“æœä¼šä»¥ç±»ä¼¼ AgentBusters-Leaderboard çš„æ ¼å¼ä¿å­˜ï¼Œä¾¿äºå¯¹æ¯”åˆ†æã€‚

Usage:
    # è¿è¡Œç‰¹å®šå®éªŒ
    python scripts/run_experiments.py --experiment model_comparison
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    python scripts/run_experiments.py --all
    
    # åªè¿è¡Œå®éªŒä¸­çš„ç‰¹å®šé…ç½®
    python scripts/run_experiments.py --experiment model_comparison --config-id llama3.1-70b
    
    # æŒ‡å®šè¾“å‡ºç›®å½•
    python scripts/run_experiments.py --experiment scale_comparison --output-dir results/scale_test
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨å®éªŒ
    python scripts/run_experiments.py --list

Examples:
    # å¿«é€Ÿæµ‹è¯•ï¼šåªè¿è¡Œä¸€ä¸ªå°è§„æ¨¡é…ç½®
    python scripts/run_experiments.py --experiment scale_comparison --config-id scale-10
    
    # å®Œæ•´æ¨¡å‹å¯¹æ¯”
    python scripts/run_experiments.py --experiment model_comparison
"""

import argparse
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

try:
    import yaml
except ImportError:
    print("Error: pyyaml required. Install with: pip install pyyaml")
    sys.exit(1)


def load_experiment_configs(config_path: str) -> dict:
    """åŠ è½½å®éªŒé…ç½®æ–‡ä»¶"""
    path = Path(config_path)
    if not path.exists():
        print(f"Error: Config file not found: {config_path}")
        sys.exit(1)
    
    with open(path) as f:
        return yaml.safe_load(f)


def update_env_file(
    env_path: str,
    llm_model: str,
    llm_api_base: Optional[str] = None,
    llm_temperature: float = 0.0,
    llm_provider: str = "openai",
) -> dict:
    """æ›´æ–° .env æ–‡ä»¶ä¸­çš„ LLM é…ç½®å¹¶è¿”å›åŸå§‹å€¼"""
    from dotenv import dotenv_values, set_key
    
    path = Path(env_path)
    original = dotenv_values(path) if path.exists() else {}
    
    # è®¾ç½®æ–°å€¼
    set_key(path, "LLM_MODEL", llm_model)
    set_key(path, "PURPLE_LLM_TEMPERATURE", str(llm_temperature))
    set_key(path, "LLM_PROVIDER", llm_provider)
    
    if llm_api_base:
        set_key(path, "OPENAI_API_BASE", llm_api_base)
        set_key(path, "OPENAI_BASE_URL", llm_api_base)
    
    return original


def run_single_experiment(
    config_id: str,
    llm_model: str,
    eval_config: str,
    num_tasks: int,
    output_dir: str,
    llm_api_base: Optional[str] = None,
    llm_temperature: float = 0.0,
    llm_provider: str = "openai",
    green_url: str = "http://localhost:9109",
    purple_url: str = "http://localhost:9110",
    timeout: int = 3600,
    dry_run: bool = False,
) -> dict:
    """è¿è¡Œå•ä¸ªå®éªŒé…ç½®"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = Path(output_dir) / f"{config_id}_{timestamp}.json"
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ Running experiment: {config_id}")
    print(f"{'='*70}")
    print(f"  Model:      {llm_model}")
    print(f"  API Base:   {llm_api_base or 'default'}")
    print(f"  Config:     {eval_config}")
    print(f"  Tasks:      {num_tasks}")
    print(f"  Timeout:    {timeout}s")
    print(f"  Output:     {output_file}")
    print()
    
    if dry_run:
        print("  [DRY RUN - skipping actual execution]")
        return {
            "config_id": config_id,
            "llm_model": llm_model,
            "num_tasks": num_tasks,
            "output_file": str(output_file),
            "success": True,
            "dry_run": True,
            "timestamp": timestamp,
        }
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env["LLM_MODEL"] = llm_model
    env["PURPLE_LLM_TEMPERATURE"] = str(llm_temperature)
    env["LLM_PROVIDER"] = llm_provider
    
    if llm_api_base:
        env["OPENAI_API_BASE"] = llm_api_base
        env["OPENAI_BASE_URL"] = llm_api_base
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        sys.executable,
        "scripts/run_a2a_eval.py",
        "--green-url", green_url,
        "--purple-url", purple_url,
        "--num-tasks", str(num_tasks),
        "--timeout", str(timeout),
        "-v",
        "-o", str(output_file),
    ]
    
    start_time = time.time()
    
    try:
        result = subprocess.run(
            cmd,
            env=env,
            capture_output=False,
            cwd=Path(__file__).parent.parent,
        )
        success = result.returncode == 0
    except Exception as e:
        print(f"  âŒ Error: {e}")
        success = False
    
    elapsed = time.time() - start_time
    
    result_info = {
        "config_id": config_id,
        "llm_model": llm_model,
        "llm_api_base": llm_api_base,
        "llm_temperature": llm_temperature,
        "eval_config": eval_config,
        "num_tasks": num_tasks,
        "output_file": str(output_file),
        "success": success,
        "elapsed_seconds": round(elapsed, 2),
        "timestamp": timestamp,
    }
    
    if success:
        print(f"  âœ… Completed in {elapsed/60:.1f} minutes")
    else:
        print(f"  âŒ Failed after {elapsed/60:.1f} minutes")
    
    return result_info


def run_experiment_suite(
    experiment_name: str,
    configs: list,
    output_dir: str,
    config_id_filter: Optional[str] = None,
    dry_run: bool = False,
    **kwargs,
) -> list:
    """è¿è¡Œä¸€ç»„å®éªŒ"""
    
    results = []
    total = len(configs)
    
    for i, config in enumerate(configs, 1):
        cid = config["id"]
        
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šé…ç½®ï¼Œåªè¿è¡ŒåŒ¹é…çš„
        if config_id_filter and cid != config_id_filter:
            continue
        
        print(f"\n[{i}/{total}] {experiment_name} / {cid}")
        
        result = run_single_experiment(
            config_id=cid,
            llm_model=config.get("llm_model", os.getenv("LLM_MODEL", "gpt-4o")),
            eval_config=config.get("eval_config", "config/eval_config.yaml"),
            num_tasks=config.get("num_tasks", 100),
            output_dir=output_dir,
            llm_api_base=config.get("llm_api_base"),
            llm_temperature=config.get("llm_temperature", 0.0),
            llm_provider=config.get("llm_provider", "openai"),
            dry_run=dry_run,
            **kwargs,
        )
        results.append(result)
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        summary_file = Path(output_dir) / f"{experiment_name}_progress.json"
        with open(summary_file, "w") as f:
            json.dump(results, f, indent=2)
    
    return results


def list_experiments(config: dict):
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å®éªŒ"""
    print("\n" + "="*70)
    print("Available Experiments")
    print("="*70)
    
    for exp in config.get("experiments", []):
        print(f"\nğŸ“‹ {exp['name']}")
        print(f"   {exp['description']}")
        print("   Configurations:")
        for cfg in exp.get("configs", []):
            model = cfg.get("llm_model", "default")
            tasks = cfg.get("num_tasks", "?")
            print(f"     - {cfg['id']}: {model} ({tasks} tasks)")


def main():
    parser = argparse.ArgumentParser(
        description="Run multiple experiment configurations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument(
        "--config",
        default="experiments/experiment_configs.yaml",
        help="Path to experiment configs YAML",
    )
    parser.add_argument(
        "--experiment",
        help="Specific experiment to run (e.g., model_comparison)",
    )
    parser.add_argument(
        "--config-id",
        help="Only run specific config within experiment",
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Run all experiments",
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="List available experiments and exit",
    )
    parser.add_argument(
        "--output-dir",
        default="results/experiments",
        help="Directory to save results",
    )
    parser.add_argument(
        "--green-url",
        default="http://localhost:9109",
        help="Green Agent URL",
    )
    parser.add_argument(
        "--purple-url",
        default="http://localhost:9110",
        help="Purple Agent URL",
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=3600,
        help="Timeout per experiment in seconds",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Print what would be run without executing",
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_experiment_configs(args.config)
    
    # åˆ—å‡ºå®éªŒ
    if args.list:
        list_experiments(config)
        return
    
    # éªŒè¯å‚æ•°
    if not args.all and not args.experiment:
        print("Error: Specify --experiment <name> or --all")
        print("Use --list to see available experiments")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    all_results = []
    experiments_run = 0
    
    for experiment in config.get("experiments", []):
        if args.all or args.experiment == experiment["name"]:
            print(f"\n{'#'*70}")
            print(f"# Experiment: {experiment['name']}")
            print(f"# {experiment['description']}")
            print(f"{'#'*70}")
            
            results = run_experiment_suite(
                experiment["name"],
                experiment["configs"],
                args.output_dir,
                config_id_filter=args.config_id,
                dry_run=args.dry_run,
                green_url=args.green_url,
                purple_url=args.purple_url,
                timeout=args.timeout,
            )
            all_results.extend(results)
            experiments_run += 1
    
    if experiments_run == 0:
        print(f"Error: Experiment '{args.experiment}' not found")
        print("Use --list to see available experiments")
        sys.exit(1)
    
    # ä¿å­˜æœ€ç»ˆæ±‡æ€»
    summary_file = Path(args.output_dir) / "experiment_summary.json"
    with open(summary_file, "w") as f:
        json.dump({
            "generated_at": datetime.now().isoformat(),
            "experiments_run": experiments_run,
            "total_configs": len(all_results),
            "successful": sum(1 for r in all_results if r.get("success")),
            "results": all_results,
        }, f, indent=2)
    
    # æ‰“å°æ‘˜è¦
    print(f"\n{'='*70}")
    print("EXPERIMENT SUMMARY")
    print("="*70)
    
    successful = sum(1 for r in all_results if r.get("success"))
    failed = len(all_results) - successful
    
    print(f"  Total configs run: {len(all_results)}")
    print(f"  Successful: {successful}")
    print(f"  Failed: {failed}")
    print(f"\nâœ… Summary saved to: {summary_file}")
    
    if failed > 0:
        print("\nâš ï¸  Some experiments failed. Check individual logs for details.")
        sys.exit(1)


if __name__ == "__main__":
    main()
