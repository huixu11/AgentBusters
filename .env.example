# CIO-Agent FAB++ Environment Configuration
# Copy this file to .env and fill in your values
# See docs/BENCHMARK_GUIDE.md for detailed usage instructions

# LLM Provider Configuration
# Options: openai, anthropic
LLM_PROVIDER=openai

# ============================================
# Option 1: Local vLLM (GPU Server - Recommended for benchmarking)
# ============================================
# Start vLLM server first:
#   vllm serve meta-llama/Llama-3.1-70B-Instruct --port 8000
# Then uncomment:
# OPENAI_API_KEY=dummy
# OPENAI_API_BASE=http://localhost:8000/v1
# OPENAI_BASE_URL=http://localhost:8000/v1
# LLM_MODEL=meta-llama/Llama-3.1-70B-Instruct

# Alternative open-source models for vLLM:
# LLM_MODEL=meta-llama/Llama-3.1-8B-Instruct     # Smaller, faster
# LLM_MODEL=Qwen/Qwen2.5-72B-Instruct            # Strong Chinese support
# LLM_MODEL=Qwen/Qwen2.5-32B-Instruct            # Medium size
# LLM_MODEL=mistralai/Mixtral-8x22B-Instruct-v0.1
# LLM_MODEL=deepseek-ai/DeepSeek-V2.5

# ============================================
# Option 2: OpenRouter API (Access many open-source models)
# ============================================
# Get API key from https://openrouter.ai/keys
# OPENAI_API_KEY=sk-or-v1-xxxxxxxxxxxxx
# OPENAI_API_BASE=https://openrouter.ai/api/v1
# LLM_MODEL=meta-llama/llama-3.1-70b-instruct
# Alternative models via OpenRouter:
# LLM_MODEL=qwen/qwen-2.5-72b-instruct
# LLM_MODEL=deepseek/deepseek-chat
# LLM_MODEL=deepseek/deepseek-reasoner
# LLM_MODEL=mistralai/mixtral-8x22b-instruct

# ============================================
# Option 3: OpenAI API
# ============================================
OPENAI_API_KEY=sk-...
LLM_MODEL=gpt-4o

# ============================================
# Option 4: Anthropic API
# ============================================
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL=claude-sonnet-4-20250514

# Purple Agent LLM Temperature
# Set to 0.0 for reproducible benchmark results (deterministic outputs)
# Higher values (e.g., 0.3) allow more creative responses but reduce reproducibility
PURPLE_LLM_TEMPERATURE=0.0

# Database Configuration (for persistent task storage)
# SQLite (local dev): use separate files to avoid lock conflicts
DATABASE_URL=sqlite+aiosqlite:///tasks.db
PURPLE_DATABASE_URL=sqlite+aiosqlite:///purple_tasks.db
# PostgreSQL (production): can use same DB since it supports concurrent writes
# DATABASE_URL=postgresql+asyncpg://user:pass@localhost/agentbusters
# PURPLE_DATABASE_URL=postgresql+asyncpg://user:pass@localhost/agentbusters

# MCP Server Configuration
# Purple Agent can use remote MCP servers (via HTTP) or in-process MCP (if URLs unset).
# RECOMMENDED: Use in-process MCP (faster, simpler, no external processes needed)
# Just leave these commented out:
# MCP_EDGAR_URL=http://localhost:8101
# MCP_YFINANCE_URL=http://localhost:8102
# MCP_SANDBOX_URL=http://localhost:8103

# Evaluation Settings
# LLM-as-judge for dataset evaluators (bizfinbench/public_csv)
# EVAL_USE_LLM=true
# EVAL_LLM_MODEL=gpt-4o-mini
# EVAL_LLM_TEMPERATURE=0.0

# Private Evaluation Data Repository (for hidden crypto scenarios)
# EVAL_DATA_REPO=yxc20089/agentbusters-eval-data
# EVAL_DATA_PAT=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Enable temporal locking (prevents look-ahead bias)
TEMPORAL_LOCK_ENABLED=true

# Noise injection ratio for distractor documents (0.0 to 1.0)
NOISE_INJECTION_RATIO=0.3

# Enable token cost tracking
TOKEN_TRACKING_ENABLED=true

# Default simulation date (YYYY-MM-DD)
# If not set, defaults to 1 year before current date
# SIMULATION_DATE=2023-01-01

# Enable adversarial debate phase
DEBATE_ENABLED=true

# Enable Alpha Score calculation
ALPHA_SCORE_ENABLED=true

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# AlphaVantage API Key (get from https://www.alphavantage.co/support/#api-key)
# Required for synthetic benchmark generation
ALPHAVANTAGE_API_KEY=your_api_key_here
